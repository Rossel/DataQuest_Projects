{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Guided Project: Exploring Hacker News Posts\n",
    "\n",
    "![hacker news logo](https://s3.amazonaws.com/dq-content/354/hacker_news.jpg)\n",
    "\n",
    "In this project, we'll work with a data set of submissions to popular technology site [Hacker News](https://news.ycombinator.com/). Hacker News is a site started by the startup incubator [Y Combinator](https://www.ycombinator.com/), where user-submitted stories (known as \"posts\") are voted and commented upon, in a similar way as as reddit. Hacker News is highly popular in technology and startup circles, and posts that make it to the top of Hacker News' listings can get hundreds of thousands of visitors as a result.\n",
    "\n",
    "You can find the data set [here](https://www.kaggle.com/hacker-news/hacker-news-posts), but note that it has been reduced from almost 300,000 rows to approximately 20,000 rows by removing all submissions that did not receive any comments, and then randomly sampling from the remaining submissions. Below are descriptions of the columns:\n",
    "\n",
    "* `id`: The unique identifier from Hacker News for the post\n",
    "* `title`: The title of the post\n",
    "* `url`: The URL that the posts links to, if it the post has a URL\n",
    "* `num_points`: The number of points the post acquired, calculated as the total number of upvotes minus the total number of downvotes\n",
    "* `num_comments`: The number of comments that were made on the post\n",
    "* `author`: The username of the person who submitted the post\n",
    "* `created_at`: The date and time at which the post was submitted\n",
    "\n",
    "\n",
    "We're specifically interested in posts whose titles begin with either `Ask HN` or `Show HN`. Users submit `Ask HN` posts to ask the Hacker News community a specific question. Below are a couple examples:\n",
    "\n",
    "* Ask HN: How to improve my personal website?\n",
    "* Ask HN: Am I the only one outraged by Twitter shutting down share counts?\n",
    "* Ask HN: Any recent changes to CSS that broke mobile?\n",
    "\n",
    "\n",
    "Likewise, users submit `Show HN` posts to show the Hacker News community a project, product, or just generally something interesting. Below are a couple of examples:\n",
    "\n",
    "* Show HN: Wio Link  ESP8266 Based Web of Things Hardware Development Platform'\n",
    "* Show HN: Something pointless I made\n",
    "* Show HN: Shanhu.io, a programming playground powered by e8vm\n",
    "\n",
    "We'll compare these two types of posts to determine the following:\n",
    "\n",
    "* Do `Ask HN` or `Show HN` receive more comments on average?\n",
    "* Do posts created at a certain time receive more comments on average?\n",
    "\n",
    "Let's start by importing the libraries we need and reading the data set into a list of lists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open and explore the data set\n",
    "\n",
    "First, let's open the data set. As mentioned before, it can be found on Kaggle using [this link](https://www.kaggle.com/hacker-news/hacker-news-posts). For ease of use, I have renamed the file to \"hacker_news.csv\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from csv import reader\n",
    "opened_file = open(\"hacker_news.csv\")\n",
    "read_file = reader(opened_file)\n",
    "hn = list(read_file) # a list of lists of the full data set\n",
    "headers = hn[0]      # to display the headers separately\n",
    "hn_rows = hn[1:]     # to access the data without the headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how many entries there are in this data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 293,119 entries in the data set.\n"
     ]
    }
   ],
   "source": [
    "print(\"There are {:,.0f} entries in the data set.\".format(len(hn_rows)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also explore the colums for this data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 7 columns:\n",
      "['id', 'title', 'url', 'num_points', 'num_comments', 'author', 'created_at']\n"
     ]
    }
   ],
   "source": [
    "print(\"There are {} columns:\".format(len(headers)))\n",
    "print(headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first five rows look as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "['12579008', 'You have two days to comment if you want stem cells to be classified as your own', 'http://www.regulations.gov/document?D=FDA-2015-D-3719-0018', '1', '0', 'altstar', '9/26/2016 3:26']\n",
      "\n",
      "\n",
      "['12579005', 'SQLAR  the SQLite Archiver', 'https://www.sqlite.org/sqlar/doc/trunk/README.md', '1', '0', 'blacksqr', '9/26/2016 3:24']\n",
      "\n",
      "\n",
      "['12578997', 'What if we just printed a flatscreen television on the side of our boxes?', 'https://medium.com/vanmoof/our-secrets-out-f21c1f03fdc8#.ietxmez43', '1', '0', 'pavel_lishin', '9/26/2016 3:19']\n",
      "\n",
      "\n",
      "['12578989', 'algorithmic music', 'http://cacm.acm.org/magazines/2011/7/109891-algorithmic-composition/fulltext', '1', '0', 'poindontcare', '9/26/2016 3:16']\n",
      "\n",
      "\n",
      "['12578979', 'How the Data Vault Enables the Next-Gen Data Warehouse and Data Lake', 'https://www.talend.com/blog/2016/05/12/talend-and-Â\\x93the-data-vaultÂ\\x94', '1', '0', 'markgainor1', '9/26/2016 3:14']\n"
     ]
    }
   ],
   "source": [
    "for row in hn_rows[:5]:\n",
    "    print('\\n')\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting 'Ask HN' and 'Show HN' posts\n",
    "Now that we've removed the headers from `hn` (the list of lists of the full data set), we're ready to filter our data further. Since we're mostly concerned with post titles beginning with `Ask HN` or `Show HN`, we'll create new lists of lists containing just the data for those titles, as well as a third list for other posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 9,139 number of 'Ask HN' posts.\n",
      "There are 10,158 number of 'Show HN' posts.\n",
      "There are 273,823 number of posts in the other category.\n"
     ]
    }
   ],
   "source": [
    "ask_posts = []\n",
    "show_posts = []\n",
    "other_posts = []\n",
    "\n",
    "for row in hn:\n",
    "    title = row[1]\n",
    "    if title.lower().startswith('ask hn'):\n",
    "        ask_posts.append(row)\n",
    "    elif title.lower().startswith('show hn'):\n",
    "        show_posts.append(row)\n",
    "    else:\n",
    "        other_posts.append(row)\n",
    "        \n",
    "print(\"There are {:,.0f} number of 'Ask HN' posts.\".format(len(ask_posts)))\n",
    "print(\"There are {:,.0f} number of 'Show HN' posts.\".format(len(show_posts)))\n",
    "print(\"There are {:,.0f} number of posts in the other category.\".format(len(other_posts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Calculating the average number of comments for 'Ask HN' and 'Show HN' posts\n",
    "\n",
    "Now that we have separated the `Ask HN` and the `Show HN` posts into two list of lists, we will determine which of these receive more comments on average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Ask HN' posts have an average of 10.39 comments per post.\n",
      "'Show HN' posts have an average of 4.89 comments per post.\n"
     ]
    }
   ],
   "source": [
    "total_ask_comments = 0\n",
    "total_show_comments = 0\n",
    "\n",
    "for row in ask_posts:\n",
    "    num_ask_comments = int(row[4])\n",
    "    total_ask_comments += num_ask_comments \n",
    "    \n",
    "for row in show_posts:\n",
    "    num_show_comments = int(row[4])\n",
    "    total_show_comments += num_show_comments \n",
    "    \n",
    "avg_ask_comments = round(total_ask_comments / len(ask_posts), 2)\n",
    "avg_show_comments = round(total_show_comments / len(show_posts), 2)\n",
    "\n",
    "print(\"'Ask HN' posts have an average of {} comments per post.\".format(avg_ask_comments))\n",
    "print(\"'Show HN' posts have an average of {} comments per post.\".format(avg_show_comments))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the average number of comments is markedly higher in the `Ask HN` posts than in the `Show HN` posts, we can conclude that posts containing questions receive more engagement from the Hacker News community. \n",
    "\n",
    "Since `Ask HN` posts are more likely to receive comments, we'll focus our remaining analysis just on these posts. Next, we'll determine if posts created at a certain time are more likely to attract comments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Finding the amount of 'Ask HN' posts and comments by hour created"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "First, we will calculate the amount of `Ask HN` posts created in each hour of the day, along with the number of comments they receive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_list = []\n",
    "\n",
    "for row in ask_posts:\n",
    "    created_at = row[6]\n",
    "    n_comments = int(row[4])\n",
    "    results_list.append([created_at, n_comments])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will make two dictionaries to calculate the amount of posts and their number of comments. We will use the `datetime` module to work with the data in the `created_at` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts by hour:\n",
      "{'02': 269, '01': 282, '22': 383, '21': 518, '19': 552, '17': 587, '15': 646, '14': 513, '13': 444, '11': 312, '10': 282, '09': 222, '07': 226, '03': 271, '23': 343, '20': 510, '16': 579, '08': 257, '00': 301, '18': 614, '12': 342, '04': 243, '06': 234, '05': 209}\n",
      "Comments by hour:\n",
      "{'02': 2996, '01': 2089, '22': 3372, '21': 4500, '19': 3954, '17': 5547, '15': 18525, '14': 4972, '13': 7245, '11': 2797, '10': 3013, '09': 1477, '07': 1585, '03': 2154, '23': 2297, '20': 4462, '16': 4466, '08': 2362, '00': 2277, '18': 4877, '12': 4234, '04': 2360, '06': 1587, '05': 1838}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import datetime as dt\n",
    "\n",
    "counts_by_hour = {}\n",
    "comments_by_hour = {}\n",
    "date_format = \"%m/%d/%Y %H:%M\"\n",
    "\n",
    "for row in results_list:\n",
    "    date = dt.datetime.strptime(row[0], date_format)\n",
    "    hour = date.strftime(\"%H\")\n",
    "    n_comments = row[1]\n",
    "    if hour in counts_by_hour:\n",
    "        counts_by_hour[hour] += 1\n",
    "        comments_by_hour[hour] += n_comments\n",
    "    else:\n",
    "        counts_by_hour[hour] = 1\n",
    "        comments_by_hour[hour] = n_comments\n",
    "\n",
    "comment= 'Counts by hour:\\n{}\\nComments by hour:\\n{}\\n' \n",
    "print(comment.format(counts_by_hour, comments_by_hour))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will calculate the average number of comments `Ask HN` posts receive by hour created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating the average number of comments for 'Ask HN' posts by hour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Using the two dictionaries created above, we will now calculate the average number of comments for `Ask HN` posts created during each hour of the day. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['00', 7.56],\n",
       " ['01', 7.41],\n",
       " ['02', 11.14],\n",
       " ['03', 7.95],\n",
       " ['04', 9.71],\n",
       " ['05', 8.79],\n",
       " ['06', 6.78],\n",
       " ['07', 7.01],\n",
       " ['08', 9.19],\n",
       " ['09', 6.65],\n",
       " ['10', 10.68],\n",
       " ['11', 8.96],\n",
       " ['12', 12.38],\n",
       " ['13', 16.32],\n",
       " ['14', 9.69],\n",
       " ['15', 28.68],\n",
       " ['16', 7.71],\n",
       " ['17', 9.45],\n",
       " ['18', 7.94],\n",
       " ['19', 7.16],\n",
       " ['20', 8.75],\n",
       " ['21', 8.69],\n",
       " ['22', 8.8],\n",
       " ['23', 6.7]]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_by_hour = []\n",
    "\n",
    "for hour in counts_by_hour:\n",
    "    avg = round(comments_by_hour[hour]/counts_by_hour[hour], 2)\n",
    "    avg_by_hour.append([hour, avg])\n",
    "\n",
    "sorted(avg_by_hour)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we now have the results we need, this format makes it hard to identify the hours with the highest values. \n",
    "The easiest way to solve this would be to change the order of items in the list and then sort on the `avg` item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[28.68, '15'],\n",
       " [16.32, '13'],\n",
       " [12.38, '12'],\n",
       " [11.14, '02'],\n",
       " [10.68, '10'],\n",
       " [9.71, '04'],\n",
       " [9.69, '14'],\n",
       " [9.45, '17'],\n",
       " [9.19, '08'],\n",
       " [8.96, '11'],\n",
       " [8.8, '22'],\n",
       " [8.79, '05'],\n",
       " [8.75, '20'],\n",
       " [8.69, '21'],\n",
       " [7.95, '03'],\n",
       " [7.94, '18'],\n",
       " [7.71, '16'],\n",
       " [7.56, '00'],\n",
       " [7.41, '01'],\n",
       " [7.16, '19'],\n",
       " [7.01, '07'],\n",
       " [6.78, '06'],\n",
       " [6.7, '23'],\n",
       " [6.65, '09']]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_by_hour_sorted = []\n",
    "\n",
    "for hour in counts_by_hour:\n",
    "    avg = round(comments_by_hour[hour]/counts_by_hour[hour], 2)\n",
    "    avg_by_hour_sorted.append([avg, hour])\n",
    "\n",
    "avg_by_hour_s = sorted(avg_by_hour_sorted, reverse = True)\n",
    "avg_by_hour_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's finish by sorting the list of lists and printing the five highest values in a format that's easier to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Hours for 'Ask HN' Posts Comments:\n",
      "15:00: 28.68 average comments per post\n",
      "13:00: 16.32 average comments per post\n",
      "12:00: 12.38 average comments per post\n",
      "02:00: 11.14 average comments per post\n",
      "10:00: 10.68 average comments per post\n"
     ]
    }
   ],
   "source": [
    "print(\"Top 5 Hours for 'Ask HN' Posts Comments:\")\n",
    "\n",
    "for row in avg_by_hour_s[:5]:\n",
    "    date = dt.datetime.strptime(row[1], \"%H\")\n",
    "    time = date.strftime(\"%H:%M\")\n",
    "    print(\"{}: {:.2f} average comments per post\".format(time, row[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "From the analysis above, we can conclude that `Ask HN` posts have received the most comments on average between 15:00-16:00 GMT. Another time slot on which many comments have been made was between 12:00-14:00 GMT. \n",
    "\n",
    "Fewest comments were made on average on `Ask HN` posts that were placed between 6:00-8:00 GMT and between 00:00-02:00 GMT. It is advised to avoid these time slots when trying to obtain the maximum amount of comments, based on averages from the dataset. A remarkable outlier is 2:00, which is the timeslot that receives the 4th most comments. Perhaps, this is because the comments are posted by users from different time zones.\n",
    "\n",
    "Please note that these times are expressed in GMT and need to be converted to one's local time zone. This [site](http://www.worldtimeserver.com) can help you with this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
